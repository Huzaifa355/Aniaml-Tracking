{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40accbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 719 files belonging to 3 classes.\n",
      "Found 181 files belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\white\\AppData\\Local\\Temp\\ipykernel_16568\\751258837.py:34: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base = tf.keras.applications.MobileNet(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 360ms/step - accuracy: 0.4691 - loss: 1.9329 - val_accuracy: 0.8619 - val_loss: 0.3936\n",
      "Epoch 2/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.7865 - loss: 0.5552 - val_accuracy: 0.8950 - val_loss: 0.2643\n",
      "Epoch 3/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.8602 - loss: 0.3582 - val_accuracy: 0.9061 - val_loss: 0.2199\n",
      "Epoch 4/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.8940 - loss: 0.2856 - val_accuracy: 0.9116 - val_loss: 0.2033\n",
      "Epoch 5/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9106 - loss: 0.2411 - val_accuracy: 0.9227 - val_loss: 0.1813\n",
      "Epoch 1/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 195ms/step - accuracy: 0.7624 - loss: 0.6749 - val_accuracy: 0.9337 - val_loss: 0.1718\n",
      "Epoch 2/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.8744 - loss: 0.3629 - val_accuracy: 0.9448 - val_loss: 0.1687\n",
      "Epoch 3/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 0.8676 - loss: 0.3441 - val_accuracy: 0.9392 - val_loss: 0.1577\n",
      "Epoch 4/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.8967 - loss: 0.3041 - val_accuracy: 0.9337 - val_loss: 0.1592\n",
      "Epoch 5/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.9204 - loss: 0.2322 - val_accuracy: 0.9503 - val_loss: 0.1410\n",
      "Epoch 6/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9466 - loss: 0.1552 - val_accuracy: 0.9503 - val_loss: 0.1425\n",
      "Epoch 7/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.9616 - loss: 0.1284 - val_accuracy: 0.9503 - val_loss: 0.1365\n",
      "Epoch 8/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.9621 - loss: 0.1241 - val_accuracy: 0.9613 - val_loss: 0.1292\n",
      "Epoch 9/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.9744 - loss: 0.0924 - val_accuracy: 0.9448 - val_loss: 0.1201\n",
      "Epoch 10/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.9581 - loss: 0.0945 - val_accuracy: 0.9613 - val_loss: 0.1165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done — MobileNetV1 training finished.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_preprocess\n",
    "\n",
    "# ——— your folders ———\n",
    "TRAIN_DIR = r'D:\\iot project\\train'\n",
    "VAL_DIR   = r'D:\\iot project\\val'\n",
    "IMG_SIZE  = (96, 96)\n",
    "BATCH     = 32\n",
    "NUM_CLASSES = 3  # cow, camel, goat\n",
    "\n",
    "# 1. Datasets\n",
    "train_ds = image_dataset_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    label_mode='int',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ").map(lambda x, y: (mobilenet_preprocess(x), y)) \\\n",
    " .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    VAL_DIR,\n",
    "    label_mode='int',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False\n",
    ").map(lambda x, y: (mobilenet_preprocess(x), y)) \\\n",
    " .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 2. Build MobileNetV1-based model\n",
    "base = tf.keras.applications.MobileNet(\n",
    "    input_shape=(*IMG_SIZE, 3),\n",
    "    alpha=0.25,\n",
    "    weights='imagenet',     # transfer learn from ImageNet\n",
    "    include_top=False\n",
    ")\n",
    "base.trainable = False     # freeze for initial training\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(*IMG_SIZE, 3)),\n",
    "    base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax'),\n",
    "])\n",
    "\n",
    "# 3. Compile & train head\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# 4. Unfreeze some of the base for fine-tuning\n",
    "base.trainable = True\n",
    "for layer in base.layers[:50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 5. Optional: Combine histories or save model\n",
    "model.save('mobilenetv1_finetuned.h5')\n",
    "print(\"✅ Done — MobileNetV1 training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416b9172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\white\\AppData\\Local\\Temp\\tmpdo3thepj\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\white\\AppData\\Local\\Temp\\tmpdo3thepj\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\white\\AppData\\Local\\Temp\\tmpdo3thepj'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1869806412880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806413840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806414224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806414032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806412496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806415376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806415760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806416144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806415952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806413072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806417296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806417680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806418064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806417872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806413456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806419216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806419792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806418448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806419600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869806416912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810352976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810353744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810354128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810353936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810353360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810355280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810355664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810356048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810355856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810352400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810357200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810357584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810357968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810357776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810352208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810359120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810359504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810359888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810359696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810354896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810361040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810361424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810361808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810361616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810356816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810362960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810363344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810363728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810363536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810358736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810364880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810365264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810365648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810365456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810360656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810366800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810367184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810367568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810367376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810362576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810366032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810565776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810566160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810368336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810364496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810567312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810567696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810568080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810567888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810565200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810569232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810569616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810570000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810569808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810565584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810571152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810571536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810571920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810571728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810566928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810573072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810573456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810573840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810573648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810568848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810574992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810575376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810575760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810575568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810570768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810576912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810577296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810577680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810577488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810572688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810578832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810579216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810579600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810579408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810574608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810580752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810581328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810579984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810581136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810578448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810745616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810746960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810747344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810747152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810745808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810748496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810748880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810749264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810749072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810746384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810750416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810750800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810751184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810750992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810746576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810752336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810752720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810753104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810752912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810748112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810754256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810754640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810755024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810754832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810750032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810756176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810756560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810756944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810756752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810751952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810758096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810758864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810757328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1869810760208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\white\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quantization complete, saved mobilenetv1_quant.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "model = tf.keras.models.load_model(r'D:\\iot project\\mobilenetv1_finetuned.h5', compile=False)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "def rep_data_gen():\n",
    "    for _ in range(100):\n",
    "        # random float in [0,1], matching your model's float32 input\n",
    "        dummy = np.random.random_sample((1,96,96,3)).astype(np.float32)\n",
    "        yield [dummy]\n",
    "\n",
    "converter.representative_dataset = rep_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Keep the I/O as float32 so it matches your model signature\n",
    "# (the weights & activations will still be quantized to int8 internally)\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "with open('mobilenetv1_quant.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"✅ Quantization complete, saved mobilenetv1_quant.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e7dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 181 files belonging to 3 classes.\n",
      "→ Model input details: {'name': 'serving_default_input_layer_1:0', 'index': 0, 'shape': array([ 1, 96, 96,  3], dtype=int32), 'shape_signature': array([-1, 96, 96,  3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "→ Model expects dtype: <class 'numpy.float32'> shape: [ 1 96 96  3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\white\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 82.87%  (150/181)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# ——— EDIT THESE PATHS IF NEEDED ———\n",
    "TFLITE_MODEL_PATH = r'D:\\iot project\\mobilenetv1_quant.tflite'\n",
    "VAL_DIR            = r'D:\\iot project\\val'     # cow/, camel/, goat/\n",
    "BATCH_SIZE         = 32\n",
    "IMG_SIZE           = (96, 96)\n",
    "\n",
    "# 1. Load validation dataset (values in [0,255], dtype uint8)\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    VAL_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 2. Load TFLite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details  = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "print(\"→ Model input details:\", input_details)\n",
    "print(\"→ Model expects dtype:\", input_details['dtype'],\n",
    "      \"shape:\", input_details['shape'])\n",
    "\n",
    "# 3. Evaluate\n",
    "total, correct = 0, 0\n",
    "\n",
    "for batch_images, batch_labels in val_ds:\n",
    "    # Convert to numpy for feeding the interpreter\n",
    "    imgs = batch_images.numpy()           # shape (batch,128,128,3), uint8\n",
    "\n",
    "    # 3a. Preprocess depending on expected input dtype\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "        # Model wants uint8 [0..255]\n",
    "        batch_input = imgs.astype(np.uint8)\n",
    "\n",
    "    elif input_details['dtype'] == np.float32:\n",
    "        # Model wants float32 in [0..1]\n",
    "        batch_input = (imgs / 255.0).astype(np.float32)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported input dtype: {input_details['dtype']}\")\n",
    "\n",
    "    # 3b. Run per-image inference\n",
    "    for img, true_label in zip(batch_input, batch_labels.numpy()):\n",
    "        # Expand dims to match [1, h, w, c]\n",
    "        interpreter.set_tensor(input_details['index'], np.expand_dims(img, 0))\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details['index'])[0]  # e.g. shape [3]\n",
    "        pred_label = np.argmax(output)\n",
    "\n",
    "        if pred_label == true_label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "# 4. Report\n",
    "acc = 100 * correct / total if total else 0\n",
    "print(f'Validation accuracy: {acc:.2f}%  ({correct}/{total})')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
