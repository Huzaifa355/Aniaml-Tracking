{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40accbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 719 files belonging to 3 classes.\n",
      "Found 181 files belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\white\\AppData\\Local\\Temp\\ipykernel_16724\\3211690950.py:34: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base = tf.keras.applications.MobileNet(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 454ms/step - accuracy: 0.4239 - loss: 3.3818 - val_accuracy: 0.8398 - val_loss: 0.4641\n",
      "Epoch 2/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.7659 - loss: 0.5582 - val_accuracy: 0.8840 - val_loss: 0.3107\n",
      "Epoch 3/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.8802 - loss: 0.3214 - val_accuracy: 0.9227 - val_loss: 0.2172\n",
      "Epoch 4/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9141 - loss: 0.2360 - val_accuracy: 0.9282 - val_loss: 0.1811\n",
      "Epoch 5/5\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9354 - loss: 0.1909 - val_accuracy: 0.9669 - val_loss: 0.1409\n",
      "Epoch 1/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 267ms/step - accuracy: 0.8207 - loss: 0.4337 - val_accuracy: 0.9558 - val_loss: 0.1394\n",
      "Epoch 2/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.9178 - loss: 0.2378 - val_accuracy: 0.9613 - val_loss: 0.1413\n",
      "Epoch 3/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.8891 - loss: 0.2684 - val_accuracy: 0.9448 - val_loss: 0.1486\n",
      "Epoch 4/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - accuracy: 0.9347 - loss: 0.1834 - val_accuracy: 0.9448 - val_loss: 0.1375\n",
      "Epoch 5/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.9441 - loss: 0.1580 - val_accuracy: 0.9503 - val_loss: 0.1340\n",
      "Epoch 6/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.9523 - loss: 0.1473 - val_accuracy: 0.9558 - val_loss: 0.1114\n",
      "Epoch 7/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9705 - loss: 0.0953 - val_accuracy: 0.9613 - val_loss: 0.0975\n",
      "Epoch 8/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - accuracy: 0.9701 - loss: 0.1014 - val_accuracy: 0.9669 - val_loss: 0.0950\n",
      "Epoch 9/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.9783 - loss: 0.0810 - val_accuracy: 0.9724 - val_loss: 0.0954\n",
      "Epoch 10/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.9869 - loss: 0.0528 - val_accuracy: 0.9613 - val_loss: 0.0946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done — MobileNetV1 training finished.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_preprocess\n",
    "\n",
    "# ——— your folders ———\n",
    "TRAIN_DIR = r'D:\\iot project\\train'\n",
    "VAL_DIR   = r'D:\\iot project\\val'\n",
    "IMG_SIZE  = (96, 96)\n",
    "BATCH     = 32\n",
    "NUM_CLASSES = 3  # cow,goat,hen\n",
    "\n",
    "# 1. Datasets\n",
    "train_ds = image_dataset_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    label_mode='int',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ").map(lambda x, y: (mobilenet_preprocess(x), y)) \\\n",
    " .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    VAL_DIR,\n",
    "    label_mode='int',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False\n",
    ").map(lambda x, y: (mobilenet_preprocess(x), y)) \\\n",
    " .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 2. Build MobileNetV1-based model\n",
    "base = tf.keras.applications.MobileNet(\n",
    "    input_shape=(*IMG_SIZE, 3),\n",
    "    alpha=0.25,\n",
    "    weights='imagenet',     # transfer learn from ImageNet\n",
    "    include_top=False\n",
    ")\n",
    "base.trainable = False     # freeze for initial training\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(*IMG_SIZE, 3)),\n",
    "    base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax'),\n",
    "])\n",
    "\n",
    "# 3. Compile & train head\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# 4. Unfreeze some of the base for fine-tuning\n",
    "base.trainable = True\n",
    "for layer in base.layers[:50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 5. Optional: Combine histories or save model\n",
    "model.save('mobilenetv1_finetuned.h5')\n",
    "print(\"✅ Done — MobileNetV1 training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416b9172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\white\\AppData\\Local\\Temp\\tmpdcxjdngl\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\white\\AppData\\Local\\Temp\\tmpdcxjdngl\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\white\\AppData\\Local\\Temp\\tmpdcxjdngl'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2453449486608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449487760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449488144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449487952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449487184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449489488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449489104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449474512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449490064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449486224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453449488528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629076688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629075728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629076496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629075536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629078224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629078608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629078992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629078800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629075920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629080144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629080528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629080912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629080720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629077072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629082064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629082448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629082832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629082640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629077840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629083984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629084368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629084752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629084560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629079760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629085904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629086288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629086672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629086480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629081680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629087824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629088208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629088592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629088400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629083600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629089744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629090128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629090512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629090320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629085520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629091664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629087440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629305488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629091280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629089360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629306640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629307024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629307408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629307216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629305872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629308560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629308944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629309328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629309136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629305104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629310480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629310864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629311248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629311056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629306256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629312400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629312784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629313168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629312976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629308176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629314320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629314704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629315088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629314896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629310096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629316240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629316624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629317008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629316816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629312016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629318160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629318544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629318928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629318736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629313936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629320080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629319696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629317776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629320656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629315856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629319312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629486288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629485328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629486096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629485136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629487824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629488208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629488592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629488400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629485520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629489744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629490128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629490512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629490320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629486672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629491664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629492048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629492432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629492240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629487440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629493584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629493968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629494352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629494160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629489360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629495504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629495888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629496272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629496080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629491280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629497424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629497808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629498192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629498000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629493200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629499344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629499728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629500112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629499920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629495120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629501264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453629498960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453558133008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2453558134736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\white\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quantization complete, saved mobilenetv1_quant.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "model = tf.keras.models.load_model(r'D:\\iot project\\mobilenetv1_finetuned.h5', compile=False)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "def rep_data_gen():\n",
    "    for _ in range(100):\n",
    "        # random float in [0,1], matching your model's float32 input\n",
    "        dummy = np.random.random_sample((1,96,96,3)).astype(np.float32)\n",
    "        yield [dummy]\n",
    "\n",
    "converter.representative_dataset = rep_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Keep the I/O as float32 so it matches your model signature\n",
    "# (the weights & activations will still be quantized to int8 internally)\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "with open('mobilenetv1_quant.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"✅ Quantization complete, saved mobilenetv1_quant.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84e7dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 181 files belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\white\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Model input details: {'name': 'serving_default_input_layer_1:0', 'index': 0, 'shape': array([ 1, 96, 96,  3], dtype=int32), 'shape_signature': array([-1, 96, 96,  3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "→ Model expects dtype: <class 'numpy.float32'> shape: [ 1 96 96  3]\n",
      "Validation accuracy: 92.27%  (167/181)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# ——— EDIT THESE PATHS IF NEEDED ———\n",
    "TFLITE_MODEL_PATH = r'D:\\iot project\\mobilenetv1_quant.tflite'\n",
    "VAL_DIR            = r'D:\\iot project\\val'     # cow/, camel/, goat/\n",
    "BATCH_SIZE         = 32\n",
    "IMG_SIZE           = (96, 96)\n",
    "\n",
    "# 1. Load validation dataset (values in [0,255], dtype uint8)\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    VAL_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 2. Load TFLite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details  = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "print(\"→ Model input details:\", input_details)\n",
    "print(\"→ Model expects dtype:\", input_details['dtype'],\n",
    "      \"shape:\", input_details['shape'])\n",
    "\n",
    "# 3. Evaluate\n",
    "total, correct = 0, 0\n",
    "\n",
    "for batch_images, batch_labels in val_ds:\n",
    "    # Convert to numpy for feeding the interpreter\n",
    "    imgs = batch_images.numpy()           # shape (batch,128,128,3), uint8\n",
    "\n",
    "    # 3a. Preprocess depending on expected input dtype\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "        # Model wants uint8 [0..255]\n",
    "        batch_input = imgs.astype(np.uint8)\n",
    "\n",
    "    elif input_details['dtype'] == np.float32:\n",
    "        # Model wants float32 in [0..1]\n",
    "        batch_input = (imgs / 255.0).astype(np.float32)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported input dtype: {input_details['dtype']}\")\n",
    "\n",
    "    # 3b. Run per-image inference\n",
    "    for img, true_label in zip(batch_input, batch_labels.numpy()):\n",
    "        # Expand dims to match [1, h, w, c]\n",
    "        interpreter.set_tensor(input_details['index'], np.expand_dims(img, 0))\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details['index'])[0]  # e.g. shape [3]\n",
    "        pred_label = np.argmax(output)\n",
    "\n",
    "        if pred_label == true_label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "# 4. Report\n",
    "acc = 100 * correct / total if total else 0\n",
    "print(f'Validation accuracy: {acc:.2f}%  ({correct}/{total})')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
